################################################################################
# AN√ÅLISIS COMPLETO: IDEAS 1, 2 Y 3 - COMPLEX CONTAGIONS EN ELECCI√ìN ESCOLAR
################################################################################
# 
# OBJETIVO: Testear emp√≠ricamente la teor√≠a de complex contagions (Centola & Macy 2007)
# en el contexto de elecci√≥n escolar chilena, analizando c√≥mo la composici√≥n del
# "egohood" (vecindario del ego) modifica patrones de homofilia e influencia social
#
# HIP√ìTESIS PRINCIPALES:
# Idea 1: Homofilia SES y composici√≥n del egohood
# Idea 2: Imitaci√≥n "hacia abajo" y composici√≥n del egohood  
# Idea 3: Test de robustez causal - separar influencia social de homofilia
#
# CONEXI√ìN TE√ìRICA: 
# - Centola (2007): Complex contagions requieren "wide bridges" para propagarse
# - Granovetter (1973): "Strength of weak ties" para simple contagions
# - McAdam & Paulsen (1993): M√∫ltiples fuentes de activaci√≥n social
#
# ACTUALIZACI√ìN: Incorpora IPTW robusto para an√°lisis causal
#
################################################################################

# Cargar paquetes necesarios
required_packages <- c("dplyr", "ggplot2", "broom", "stargazer", "margins",
                       "MatchIt", "cobalt", "WeightIt", "purrr", "tidyr", 
                       "kableExtra", "gridExtra", "lmtest", "sandwich", "stringr")

print("Instalando/cargando paquetes necesarios...")
for (pkg in required_packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg)
    library(pkg, character.only = TRUE)
  }
}

################################################################################
# PASO 1: CREACI√ìN DE VARIABLES TE√ìRICAS
################################################################################

print("PASO 1: CREANDO VARIABLES PARA COMPLEX CONTAGIONS")
print("=================================================")

# Funci√≥n para crear variables de composici√≥n del egohood
create_complex_contagion_variables <- function(data) {
  
  print("Creando variables de complex contagions...")
  
  data <- data %>%
    group_by(ego_id) %>%
    mutate(
      # ============================================================================
      # IDEA 1: HOMOFILIA SES Y COMPOSICI√ìN DEL EGOHOOD
      # ============================================================================
      # H1: La homofilia SES var√≠a seg√∫n la composici√≥n del egohood
      # H1a: Efecto "salience" - homofilia m√°s fuerte cuando hay pocos vecinos del mismo SES
      # H1b: Efecto "conformity" - homofilia m√°s fuerte cuando hay muchos vecinos del mismo SES
      
      same_ses_quintil = as.numeric(ses_ego_quintil == ses_alter_quintil),
      prop_same_ses_in_egohood = mean(ses_alter_quintil == ses_ego_quintil[1], na.rm = TRUE),
      
      # Interacci√≥n cr√≠tica para testear H1a vs H1b
      interaction_homofilia = same_ses_quintil * prop_same_ses_in_egohood,
      
      # ============================================================================
      # IDEA 2: IMITACI√ìN "HACIA ABAJO" Y COMPOSICI√ìN DEL EGOHOOD
      # ============================================================================
      # H2: Estudiantes Q2-Q3 imitan vecinos de SES m√°s bajo cuando estos son escasos
      # H2a: Efecto "informaci√≥n valiosa" - vecinos de SES bajo m√°s influyentes cuando son raros
      # H2b: Efecto amplificado en barrios aventajados donde informaci√≥n de SES bajo es excepcional
      
      alter_lower_ses = as.numeric(as.numeric(ses_alter_quintil) < as.numeric(ses_ego_quintil)),
      alter_higher_ses = as.numeric(as.numeric(ses_alter_quintil) > as.numeric(ses_ego_quintil)),
      prop_lower_ses_in_egohood = mean(as.numeric(ses_alter_quintil) < as.numeric(ses_ego_quintil[1]), na.rm = TRUE),
      prop_higher_ses_in_egohood = mean(as.numeric(ses_alter_quintil) > as.numeric(ses_ego_quintil[1]), na.rm = TRUE),
      
      # Interacciones cr√≠ticas para testear H2a y H2b
      interaction_downward = alter_lower_ses * prop_lower_ses_in_egohood,
      interaction_upward = alter_higher_ses * prop_higher_ses_in_egohood,
      
      # Variable de barrios aventajados (Q4 de SES promedio)
      advantaged_neighborhood = as.numeric(ses_mean > quantile(ses_mean, 0.75, na.rm = TRUE)),
      interaction_downward_advantaged = alter_lower_ses * prop_lower_ses_in_egohood * advantaged_neighborhood,
      
      # ============================================================================
      # VARIABLES DE WIDE BRIDGES (CENTOLA 2007)
      # ============================================================================
      # Medidas directas de "bridge width" seg√∫n Centola & Macy (2007)
      
      # N√∫mero de vecinos del mismo SES (bridge width)
      bridge_width_same_ses = sum(same_ses_quintil, na.rm = TRUE),
      bridge_width_lower_ses = sum(alter_lower_ses, na.rm = TRUE),
      bridge_width_higher_ses = sum(alter_higher_ses, na.rm = TRUE),
      
      # Indicadores de "narrow bridges" (‚â§ 2 vecinos del tipo relevante)
      narrow_bridge_same_ses = as.numeric(bridge_width_same_ses <= 2),
      narrow_bridge_lower_ses = as.numeric(bridge_width_lower_ses <= 2),
      narrow_bridge_higher_ses = as.numeric(bridge_width_higher_ses <= 2),
      
      # √çndice de diversidad SES (Shannon entropy) - proxy de "complex contagion environment"
      ses_diversity_index = {
        ses_table <- table(ses_alter_quintil)
        if (length(ses_table) > 1) {
          props <- prop.table(ses_table)
          -sum(props * log(props + 1e-10))
        } else {
          0
        }
      },
      
      # ============================================================================
      # VARIABLES DE CONTROL Y AUXILIARES
      # ============================================================================
      
      # Tama√±o del egohood
      egohood_size = n(),
      
      # Indicadores de contexto socioecon√≥mico
      low_ses_neighborhood = as.numeric(ses_mean < quantile(ses_mean, 0.25, na.rm = TRUE)),
      high_diversity_neighborhood = as.numeric(ses_diversity_index > median(ses_diversity_index, na.rm = TRUE))
      
    ) %>%
    ungroup()
  
  return(data)
}

# Aplicar a ambos datasets
d_post <- create_complex_contagion_variables(d_post)
d_matric <- create_complex_contagion_variables(d_matric)

print("Variables creadas exitosamente")
print(paste("Observaciones POST-POST:", nrow(d_post)))
print(paste("Observaciones POST-MATRIC:", nrow(d_matric)))

################################################################################
# PASO 2: AN√ÅLISIS DESCRIPTIVO Y VALIDACI√ìN DE HIP√ìTESIS
################################################################################

print("\nPASO 2: AN√ÅLISIS DESCRIPTIVO")
print("============================")

# Estad√≠sticas descriptivas clave
descriptive_stats <- d_post %>%
  summarise(
    # Distribuci√≥n de variables dependientes
    prop_same_school = mean(mismo_post_post, na.rm = TRUE),
    
    # Distribuci√≥n de variables de composici√≥n del egohood
    mean_prop_same_ses = mean(prop_same_ses_in_egohood, na.rm = TRUE),
    mean_prop_lower_ses = mean(prop_lower_ses_in_egohood, na.rm = TRUE),
    mean_prop_higher_ses = mean(prop_higher_ses_in_egohood, na.rm = TRUE),
    
    # Distribuci√≥n de bridge types
    prop_narrow_bridges_same_ses = mean(narrow_bridge_same_ses, na.rm = TRUE),
    mean_bridge_width_same_ses = mean(bridge_width_same_ses, na.rm = TRUE),
    
    # Diversidad SES
    mean_ses_diversity = mean(ses_diversity_index, na.rm = TRUE),
    
    # Contextos de vecindario
    prop_advantaged_neighborhoods = mean(advantaged_neighborhood, na.rm = TRUE),
    
    .groups = "drop"
  )

print("ESTAD√çSTICAS DESCRIPTIVAS PRINCIPALES:")
print(descriptive_stats)

# An√°lisis por quintil SES
quintil_analysis <- d_post %>%
  group_by(ses_ego_quintil) %>%
  summarise(
    n_obs = n(),
    prop_same_school = mean(mismo_post_post, na.rm = TRUE),
    prop_same_ses_homofilia = mean(same_ses_quintil, na.rm = TRUE),
    prop_downward_imitation = mean(alter_lower_ses, na.rm = TRUE),
    prop_upward_imitation = mean(alter_higher_ses, na.rm = TRUE),
    mean_bridge_width = mean(bridge_width_same_ses, na.rm = TRUE),
    prop_narrow_bridges = mean(narrow_bridge_same_ses, na.rm = TRUE),
    mean_diversity = mean(ses_diversity_index, na.rm = TRUE),
    .groups = "drop"
  )

print("\nAN√ÅLISIS POR QUINTIL SES:")
print(quintil_analysis)


# Funci√≥n para crear clusters de errores est√°ndar robustos
create_dyadic_clusters <- function(data, ego_var = "ego_id", alter_var = "alter_id") {
  
  print(paste("Creando clusters para", deparse(substitute(data))))
  
  # Crear identificadores √∫nicos para ego y alter
  unique_ego_ids <- unique(data[[ego_var]])
  unique_alter_ids <- unique(data[[alter_var]])
  
  # Crear mapeo de clusters para egos
  ego_cluster_map <- setNames(
    as.character(seq_along(unique_ego_ids)), 
    as.character(unique_ego_ids)
  )
  
  # Crear mapeo de clusters para alters (continuando numeraci√≥n despu√©s de egos)
  alter_cluster_map <- setNames(
    as.character(seq_along(unique_alter_ids) + length(unique_ego_ids)), 
    as.character(unique_alter_ids)
  )
  
  # Asignar clusters a los datos
  data$cluster_ego <- ego_cluster_map[as.character(data[[ego_var]])]
  data$cluster_alter <- alter_cluster_map[as.character(data[[alter_var]])]
  
  print(paste("  - Egos √∫nicos:", length(unique_ego_ids)))
  print(paste("  - Alters √∫nicos:", length(unique_alter_ids)))
  print(paste("  - Total clusters:", length(unique_ego_ids) + length(unique_alter_ids)))
  
  return(data)
}

# Aplicar a ambos datasets
d_post <- create_dyadic_clusters(d_post)
d_matric <- create_dyadic_clusters(d_matric)

# Funci√≥n para calcular errores est√°ndar robustos para datos di√°dicos
get_dyadic_robust_se <- function(model, data_source = "post", network_type = "egocentric") {
  
  if (data_source == "post") {
    data_ref <- d_post
  } else if (data_source == "matric") {
    data_ref <- d_matric
  } else {
    stop("data_source debe ser 'post' o 'matric'")
  }
  
  if (network_type == "egocentric") {
    # REDES EGOCENTRADAS: Verificar si hay alters compartidos
    
    # Contar cu√°ntas veces aparece cada alter
    alter_counts <- table(data_ref$alter_id)
    shared_alters <- sum(alter_counts > 1)
    total_alters <- length(alter_counts)
    prop_shared <- shared_alters / total_alters
    
    print(paste("Diagn√≥stico de Red Egoc√©ntrica:"))
    print(paste("- Alters √∫nicos:", total_alters))
    print(paste("- Alters compartidos:", shared_alters))
    print(paste("- % alters compartidos:", round(prop_shared * 100, 2), "%"))
    
    if (prop_shared < 0.05) {
      # CASO A: Alters mayormente √∫nicos ‚Üí Solo clustering por ego
      print("‚Üí Usando clustering SOLO por ego (alters √∫nicos)")
      cluster_var <- data_ref$cluster_ego
      
      vcov_matrix <- sandwich::vcovCL(model, 
                                      cluster = cluster_var, 
                                      multi0 = TRUE)
    } else {
      # CASO B: Muchos alters compartidos ‚Üí Clustering di√°dico completo
      print("‚Üí Usando clustering di√°dico completo (alters compartidos)")
      cluster_matrix <- cbind(data_ref$cluster_ego, data_ref$cluster_alter)
      
      vcov_matrix <- sandwich::vcovCL(model, 
                                      cluster = cluster_matrix, 
                                      multi0 = TRUE)
    }
    
  } else if (network_type == "complete") {
    # RED COMPLETA: Clustering di√°dico como Graham
    print("‚Üí Usando clustering di√°dico completo (red completa)")
    cluster_matrix <- cbind(data_ref$cluster_ego, data_ref$cluster_alter)
    
    vcov_matrix <- sandwich::vcovCL(model, 
                                    cluster = cluster_matrix, 
                                    multi0 = TRUE)
  } else {
    stop("network_type debe ser 'egocentric' o 'complete'")
  }
  
  # Calcular coeficientes con errores robustos
  model_robust <- lmtest::coeftest(model, vcov = vcov_matrix)
  
  return(model_robust)
}



################################################################################
# PASO 3: MODELOS PRINCIPALES - IDEA 1 (HOMOFILIA)
################################################################################

print("\nPASO 3: MODELOS IDEA 1 - HOMOFILIA Y COMPOSICI√ìN DEL EGOHOOD")
print("===========================================================")

# Crear subconjuntos de datos para an√°lisis espec√≠ficos
d_post_all <- d_post  # Todos los quintiles
d_post_q2_q3 <- d_post %>% filter(ses_ego_quintil %in% c("Q2", "Q3"))
d_post_q4_q5 <- d_post %>% filter(ses_ego_quintil %in% c("Q4", "Q5 (m√°s alto)"))

# MODELO 1.1: Homofilia b√°sica - todos los quintiles
print("MODELO 1.1: Homofilia b√°sica (todos los quintiles)")
m_idea1_basic <- glm(mismo_post_post ~ 
                       same_ses_quintil
                     + prop_same_ses_in_egohood
                     + interaction_homofilia  # COEFICIENTE CLAVE
                     + ses_ego 
                     + ses_distance 
                     + I(ses_distance^2)
                     + distance
                     + factor(sexo_ego) 
                     + factor(sexo_alter) 
                     + edad_ego 
                     + edad_alter 
                     + ses_mean 
                     + ses_sd 
                     + shannon_index 
                     + network_size 
                     + alter_apply_pct
                     + factor(same_rbd) 
                     + pct_same_as_gdemates    
                     + num_school
                     + mean_math
                     + sd_math
                     + mean_reading
                     + sd_reading
                     + pct_public
                     + factor(dependency_post_alter)
                     + math_post_alter
                     + read_post_alter
                     + growth_math_post_alter
                     + growth_read_post_alter
                     + priority_student_post_alter
                     + factor(city)
                     + factor(reference_year), 
                     family = "binomial", data = d_post_all)

print("RESULTADOS MODELO 1.1:")
print("=====================")
print(summary(m_idea1_basic))

m_idea1_basic_robust <- get_dyadic_robust_se(m_idea1_basic, "post", "egocentric")


# INTERPRETACI√ìN DEL COEFICIENTE CLAVE:
coef_interaction_homofilia <- coef(m_idea1_basic)["interaction_homofilia"]
print(paste("\nCOEFICIENTE CLAVE - interaction_homofilia:", round(coef_interaction_homofilia, 4)))

if (coef_interaction_homofilia < 0) {
  print("INTERPRETACI√ìN: Homofilia M√ÅS FUERTE cuando hay MENOS vecinos del mismo SES")
  print("‚Üí Apoya HIP√ìTESIS 1a: Efecto 'salience' - informaci√≥n valiosa de fuentes escasas")
  print("‚Üí Consistente con teor√≠a de WIDE BRIDGES (Centola 2007)")
} else {
  print("INTERPRETACI√ìN: Homofilia M√ÅS FUERTE cuando hay M√ÅS vecinos del mismo SES")
  print("‚Üí Apoya HIP√ìTESIS 1b: Efecto 'conformity' - refuerzo social del grupo")
  print("‚Üí Consistente con teor√≠a de conformidad social")
}

# MODELO 1.2: Wide Bridges - validaci√≥n directa de Centola
print("\nMODELO 1.2: Wide Bridges (validaci√≥n directa de Centola 2007)")
m_wide_bridges <- glm(mismo_post_post ~ 
                        ses_ego 
                      + ses_distance 
                      + I(ses_distance^2)
                      + distance
                      # Variables clave de wide bridges
                      + same_ses_quintil
                      + narrow_bridge_same_ses
                      + same_ses_quintil * narrow_bridge_same_ses  # INTERACCI√ìN CR√çTICA
                      + ses_diversity_index
                      # Controles
                      + factor(sexo_ego) 
                      + factor(sexo_alter) 
                      + edad_ego 
                      + edad_alter 
                      + ses_mean 
                      + ses_sd 
                      + shannon_index 
                      + network_size 
                      + alter_apply_pct
                      + factor(same_rbd) 
                      + pct_same_as_gdemates    
                      + num_school
                      + mean_math
                      + sd_math
                      + mean_reading
                      + sd_reading
                      + pct_public
                      + factor(dependency_post_alter)
                      + math_post_alter
                      + read_post_alter
                      + growth_math_post_alter
                      + growth_read_post_alter
                      + priority_student_post_alter
                      + factor(city)
                      + factor(reference_year), 
                      family = "binomial", data = d_post_all)

print("RESULTADOS MODELO 1.2 - WIDE BRIDGES:")
print("====================================")
print(summary(m_wide_bridges))

m_idea1_basic_robust <- get_dyadic_robust_se(m_idea1_basic, "post", "egocentric")


# INTERPRETACI√ìN DE WIDE BRIDGES:
coef_same_ses <- coef(m_wide_bridges)["same_ses_quintil"]
coef_narrow_bridge <- coef(m_wide_bridges)["narrow_bridge_same_ses"]
coef_interaction_bridges <- coef(m_wide_bridges)["same_ses_quintil:narrow_bridge_same_ses"]
coef_diversity <- coef(m_wide_bridges)["ses_diversity_index"]

print("\nINTERPRETACI√ìN WIDE BRIDGES:")
print("===========================")
print(paste("Efecto homofilia b√°sica:", round(coef_same_ses, 4)))
print(paste("Efecto narrow bridges:", round(coef_narrow_bridge, 4)))
print(paste("Interacci√≥n CR√çTICA:", round(coef_interaction_bridges, 4)))
print(paste("Efecto diversidad SES:", round(coef_diversity, 4)))

# RESULTADO CLAVE: Seg√∫n Centola & Macy (2007)
if (coef_interaction_bridges < 0) {
  print("\nüéØ VALIDACI√ìN DE CENTOLA: Homofilia M√ÅS FUERTE con narrow bridges")
  print("‚Üí Complex contagions requieren wide bridges para propagarse")
  print("‚Üí Narrow bridges amplifican la influencia (efecto salience)")
  print("‚Üí Elecci√≥n escolar funciona como COMPLEX CONTAGION")
} else {
  print("\n‚ùå NO validaci√≥n de Centola: Narrow bridges no amplifican homofilia")
  print("‚Üí Elecci√≥n escolar puede funcionar como simple contagion")
}

if (coef_diversity < 0) {
  print("\nüéØ VALIDACI√ìN: Mayor diversidad SES reduce homofilia")
  print("‚Üí Consistent con 'complex contagion environment' (Centola 2018)")
  print("‚Üí Diversidad crea resistencia a la propagaci√≥n")
}

################################################################################
# PASO 4: MODELOS PRINCIPALES - IDEA 2 (IMITACI√ìN HACIA ABAJO)
################################################################################

print("\nPASO 4: MODELOS IDEA 2 - IMITACI√ìN HACIA ABAJO")
print("==============================================")

# MODELO 2.1: Imitaci√≥n hacia abajo - Q2 y Q3 espec√≠ficamente
print("MODELO 2.1: Imitaci√≥n hacia abajo (Q2-Q3)")
m_idea2_q2q3 <- glm(mismo_post_post ~ 
                      alter_lower_ses
                    + prop_lower_ses_in_egohood
                    + interaction_downward  # CLAVE
                    + advantaged_neighborhood
                    + interaction_downward_advantaged  # HIP√ìTESIS ESPEC√çFICA
                    + ses_ego 
                    + ses_distance 
                    + I(ses_distance^2)
                    + distance
                    + factor(sexo_ego) 
                    + factor(sexo_alter) 
                    + edad_ego 
                    + edad_alter 
                    + ses_mean 
                    + ses_sd 
                    + shannon_index 
                    + network_size 
                    + alter_apply_pct
                    + factor(same_rbd) 
                    + pct_same_as_gdemates    
                    + num_school
                    + mean_math
                    + sd_math
                    + mean_reading
                    + sd_reading
                    + pct_public
                    + factor(dependency_post_alter)
                    + math_post_alter
                    + read_post_alter
                    + growth_math_post_alter
                    + growth_read_post_alter
                    + priority_student_post_alter
                    + factor(city)
                    + factor(reference_year), 
                    family = "binomial", data = d_post_q2_q3)

print("RESULTADOS MODELO 2.1 - IMITACI√ìN HACIA ABAJO Q2-Q3:")
print("===================================================")
print(summary(m_idea2_q2q3))

# INTERPRETACI√ìN IMITACI√ìN HACIA ABAJO:
coef_alter_lower <- coef(m_idea2_q2q3)["alter_lower_ses"]
coef_prop_lower <- coef(m_idea2_q2q3)["prop_lower_ses_in_egohood"]
coef_interaction_downward <- coef(m_idea2_q2q3)["interaction_downward"]
coef_advantaged <- coef(m_idea2_q2q3)["advantaged_neighborhood"]
coef_interaction_adv <- coef(m_idea2_q2q3)["interaction_downward_advantaged"]

print("\nINTERPRETACI√ìN IMITACI√ìN HACIA ABAJO:")
print("===================================")
print(paste("Efecto b√°sico alter_lower_ses:", round(coef_alter_lower, 4)))
print(paste("Efecto prop_lower_ses_in_egohood:", round(coef_prop_lower, 4)))
print(paste("Interacci√≥n downward CR√çTICA:", round(coef_interaction_downward, 4)))
print(paste("Efecto barrios aventajados:", round(coef_advantaged, 4)))
print(paste("Interacci√≥n advantaged CR√çTICA:", round(coef_interaction_adv, 4)))

# EVALUACI√ìN DE HIP√ìTESIS 2:
if (coef_interaction_downward < 0) {
  print("\nüéØ APOYO HIP√ìTESIS 2a: Imitaci√≥n hacia abajo M√ÅS FUERTE cuando vecinos SES bajo son escasos")
  print("‚Üí Informaci√≥n valiosa de fuentes escasas")
  print("‚Üí Consistent con teor√≠a de wide bridges")
} else {
  print("\n‚ùå NO apoyo H2a: Imitaci√≥n hacia abajo no aumenta cuando vecinos SES bajo son escasos")
}

if (coef_interaction_adv > 0) {
  print("\nüéØ APOYO HIP√ìTESIS 2b: Efecto amplificado en barrios aventajados")
  print("‚Üí Informaci√≥n de SES bajo es m√°s valiosa en contextos aventajados")
} else {
  print("\n‚ùå NO apoyo H2b: Sin efecto diferencial en barrios aventajados")
}

# MODELO 2.2: Imitaci√≥n hacia arriba - an√°lisis complementario
print("\nMODELO 2.2: Imitaci√≥n hacia arriba (Q2-Q3)")
m_idea2_upward <- glm(mismo_post_post ~ 
                        alter_higher_ses
                      + prop_higher_ses_in_egohood
                      + interaction_upward  # COMPARACI√ìN CON DOWNWARD
                      + ses_ego 
                      + ses_distance 
                      + I(ses_distance^2)
                      + distance
                      + factor(sexo_ego) 
                      + factor(sexo_alter) 
                      + edad_ego 
                      + edad_alter 
                      + ses_mean 
                      + ses_sd 
                      + shannon_index 
                      + network_size 
                      + alter_apply_pct
                      + factor(same_rbd) 
                      + pct_same_as_gdemates    
                      + num_school
                      + mean_math
                      + sd_math
                      + mean_reading
                      + sd_reading
                      + pct_public
                      + factor(dependency_post_alter)
                      + math_post_alter
                      + read_post_alter
                      + growth_math_post_alter
                      + growth_read_post_alter
                      + priority_student_post_alter
                      + factor(city)
                      + factor(reference_year), 
                      family = "binomial", data = d_post_q2_q3)

print("RESULTADOS MODELO 2.2 - IMITACI√ìN HACIA ARRIBA Q2-Q3:")
print("====================================================")
print(summary(m_idea2_upward))

# COMPARACI√ìN UPWARD vs DOWNWARD:
coef_interaction_upward <- coef(m_idea2_upward)["interaction_upward"]
print(paste("\nCOMPARACI√ìN IMITACI√ìN UPWARD vs DOWNWARD:"))
print(paste("Interacci√≥n downward:", round(coef_interaction_downward, 4)))
print(paste("Interacci√≥n upward:", round(coef_interaction_upward, 4)))

if (abs(coef_interaction_upward) > abs(coef_interaction_downward)) {
  print("‚Üí IMITACI√ìN HACIA ARRIBA m√°s fuerte que hacia abajo")
  print("‚Üí Consistent con aspirational behavior")
} else {
  print("‚Üí IMITACI√ìN HACIA ABAJO comparable o m√°s fuerte que hacia arriba")
}

################################################################################
# PASO 5: AN√ÅLISIS POR SUBGRUPOS Y ROBUSTEZ
################################################################################

print("\nPASO 5: AN√ÅLISIS POR SUBGRUPOS Y ROBUSTEZ")
print("=========================================")

# An√°lisis por ciudad
print("AN√ÅLISIS POR CIUDAD:")
ciudades <- unique(d_post$city)
resultados_ciudad <- list()

for (ciudad in ciudades) {
  if (ciudad %in% c("santiago", "valparaiso", "concepcion", "laserena")) {
    
    d_ciudad <- d_post %>% filter(city == ciudad)
    
    if (nrow(d_ciudad) > 1000) {  # Solo ciudades con suficientes observaciones
      
      print(paste("Analizando ciudad:", ciudad))
      
      tryCatch({
        m_ciudad <- glm(mismo_post_post ~ 
                          same_ses_quintil * narrow_bridge_same_ses
                        + ses_diversity_index
                        + ses_ego + ses_distance + I(ses_distance^2)
                        + distance + factor(sexo_ego) + factor(sexo_alter)
                        + edad_ego + edad_alter + ses_mean + ses_sd
                        + shannon_index + network_size + alter_apply_pct
                        + factor(same_rbd) + pct_same_as_gdemates
                        + num_school + mean_math + sd_math + mean_reading + sd_reading
                        + pct_public + factor(dependency_post_alter)
                        + math_post_alter + read_post_alter
                        + growth_math_post_alter + growth_read_post_alter
                        + priority_student_post_alter + factor(reference_year), 
                        family = "binomial", data = d_ciudad)
        
        coef_interaccion <- coef(m_ciudad)["same_ses_quintil:narrow_bridge_same_ses"]
        resultados_ciudad[[ciudad]] <- list(
          ciudad = ciudad,
          n_obs = nrow(d_ciudad),
          coef_interaccion = coef_interaccion,
          se = summary(m_ciudad)$coefficients["same_ses_quintil:narrow_bridge_same_ses", "Std. Error"],
          p_value = summary(m_ciudad)$coefficients["same_ses_quintil:narrow_bridge_same_ses", "Pr(>|z|)"]
        )
        
        print(paste("  Coeficiente interacci√≥n:", round(coef_interaccion, 4)))
        
      }, error = function(e) {
        print(paste("  Error en ciudad", ciudad, ":", e$message))
      })
    }
  }
}

# An√°lisis por a√±o
print("\nAN√ÅLISIS POR A√ëO:")
a√±os <- unique(d_post$reference_year)
resultados_a√±o <- list()

for (a√±o in a√±os) {
  d_a√±o <- d_post %>% filter(reference_year == a√±o)
  
  if (nrow(d_a√±o) > 1000) {
    
    print(paste("Analizando a√±o:", a√±o))
    
    tryCatch({
      m_a√±o <- glm(mismo_post_post ~ 
                     same_ses_quintil * narrow_bridge_same_ses
                   + ses_diversity_index
                   + ses_ego + ses_distance + I(ses_distance^2)
                   + distance + factor(sexo_ego) + factor(sexo_alter)
                   + edad_ego + edad_alter + ses_mean + ses_sd
                   + shannon_index + network_size + alter_apply_pct
                   + factor(same_rbd) + pct_same_as_gdemates
                   + num_school + mean_math + sd_math + mean_reading + sd_reading
                   + pct_public + factor(dependency_post_alter)
                   + math_post_alter + read_post_alter
                   + growth_math_post_alter + growth_read_post_alter
                   + priority_student_post_alter + factor(city), 
                   family = "binomial", data = d_a√±o)
      
      coef_interaccion <- coef(m_a√±o)["same_ses_quintil:narrow_bridge_same_ses"]
      resultados_a√±o[[as.character(a√±o)]] <- list(
        a√±o = a√±o,
        n_obs = nrow(d_a√±o),
        coef_interaccion = coef_interaccion,
        se = summary(m_a√±o)$coefficients["same_ses_quintil:narrow_bridge_same_ses", "Std. Error"],
        p_value = summary(m_a√±o)$coefficients["same_ses_quintil:narrow_bridge_same_ses", "Pr(>|z|)"]
      )
      
      print(paste("  Coeficiente interacci√≥n:", round(coef_interaccion, 4)))
      
    }, error = function(e) {
      print(paste("  Error en a√±o", a√±o, ":", e$message))
    })
  }
}

################################################################################
# PASO 6: VISUALIZACIONES PUBLICATION-READY
################################################################################

print("\nPASO 6: CREANDO VISUALIZACIONES PUBLICATION-READY")
print("=================================================")

# Crear variables categ√≥ricas para visualizaciones
d_post <- d_post %>%
  mutate(
    prop_same_ses_cat = cut(prop_same_ses_in_egohood, 
                            breaks = c(0, 0.2, 0.4, 0.6, 0.8, 1), 
                            labels = c("Muy baja", "Baja", "Media", "Alta", "Muy alta"),
                            include.lowest = TRUE),
    
    prop_lower_ses_cat = cut(prop_lower_ses_in_egohood, 
                             breaks = c(0, 0.2, 0.4, 0.6, 0.8, 1), 
                             labels = c("Muy baja", "Baja", "Media", "Alta", "Muy alta"),
                             include.lowest = TRUE),
    
    bridge_type = ifelse(narrow_bridge_same_ses == 1, "Narrow Bridge", "Wide Bridge"),
    
    ses_diversity_cat = cut(ses_diversity_index, 
                            breaks = quantile(ses_diversity_index, c(0, 0.33, 0.67, 1), na.rm = TRUE),
                            labels = c("Baja diversidad", "Media diversidad", "Alta diversidad"),
                            include.lowest = TRUE)
  )

# GR√ÅFICO 1: Validaci√≥n de Wide Bridges Theory
print("Creando Gr√°fico 1: Validaci√≥n de Wide Bridges Theory")

g1_data <- d_post %>%
  filter(!is.na(bridge_type), !is.na(ses_ego_quintil), !is.na(same_ses_quintil)) %>%
  group_by(bridge_type, ses_ego_quintil) %>%
  summarise(
    homofilia_rate = mean(same_ses_quintil, na.rm = TRUE),
    n_obs = n(),
    se = sqrt(homofilia_rate * (1 - homofilia_rate) / n_obs),
    .groups = "drop"
  ) %>%
  filter(n_obs >= 100)

g1 <- g1_data %>%
  ggplot(aes(x = ses_ego_quintil, y = homofilia_rate, fill = bridge_type)) +
  geom_col(position = "dodge", alpha = 0.8) +
  geom_errorbar(aes(ymin = homofilia_rate - 1.96*se, 
                    ymax = homofilia_rate + 1.96*se),
                position = position_dodge(width = 0.9), width = 0.25) +
  labs(title = "Homofilia SES: Wide Bridges vs Narrow Bridges",
       subtitle = "Validaci√≥n emp√≠rica de Centola & Macy (2007)",
       x = "Quintil SES Ego",
       y = "Tasa de homofilia",
       fill = "Tipo de Bridge",
       caption = "Narrow Bridge = ‚â§2 vecinos del mismo SES | Wide Bridge = >2 vecinos del mismo SES\nBarras de error: IC 95%") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(size = 14, face = "bold"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 11)) +
  scale_fill_manual(values = c("Narrow Bridge" = "#E31A1C", "Wide Bridge" = "#1F78B4"))

print(g1)

# GR√ÅFICO 2: Efectos de la Diversidad SES
print("Creando Gr√°fico 2: Efectos de la Diversidad SES")

g2_data <- d_post %>%
  filter(!is.na(ses_diversity_cat), !is.na(same_ses_quintil)) %>%
  group_by(ses_diversity_cat, ses_ego_quintil) %>%
  summarise(
    homofilia_rate = mean(same_ses_quintil, na.rm = TRUE),
    n_obs = n(),
    se = sqrt(homofilia_rate * (1 - homofilia_rate) / n_obs),
    .groups = "drop"
  ) %>%
  filter(n_obs >= 100)

g2 <- g2_data %>%
  ggplot(aes(x = ses_ego_quintil, y = homofilia_rate, fill = ses_diversity_cat)) +
  geom_col(position = "dodge", alpha = 0.8) +
  geom_errorbar(aes(ymin = homofilia_rate - 1.96*se, 
                    ymax = homofilia_rate + 1.96*se),
                position = position_dodge(width = 0.9), width = 0.25) +
  labs(title = "Homofilia SES por Diversidad del Egohood",
       subtitle = "Efectos de 'Complex Contagion Environment'",
       x = "Quintil SES Ego",
       y = "Tasa de homofilia",
       fill = "Diversidad SES",
       caption = "Diversidad medida con √≠ndice de Shannon | Barras de error: IC 95%") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(size = 14, face = "bold"),
        legend.title = element_text(size = 12)) +
  scale_fill_manual(values = c("Baja diversidad" = "#FED976", 
                               "Media diversidad" = "#FEB24C", 
                               "Alta diversidad" = "#FD8D3C"))

print(g2)

# GR√ÅFICO 3: Imitaci√≥n hacia abajo vs hacia arriba
print("Creando Gr√°fico 3: Imitaci√≥n hacia abajo vs hacia arriba")

g3_data <- d_post_q2_q3 %>%
  filter(!is.na(prop_lower_ses_cat)) %>%
  group_by(prop_lower_ses_cat) %>%
  summarise(
    downward_rate = mean(alter_lower_ses, na.rm = TRUE),
    upward_rate = mean(alter_higher_ses, na.rm = TRUE),
    n_obs = n(),
    se_down = sqrt(downward_rate * (1 - downward_rate) / n_obs),
    se_up = sqrt(upward_rate * (1 - upward_rate) / n_obs),
    .groups = "drop"
  ) %>%
  pivot_longer(cols = c(downward_rate, upward_rate), 
               names_to = "direction", values_to = "rate") %>%
  mutate(
    se = ifelse(direction == "downward_rate", se_down, se_up),
    Direction = ifelse(direction == "downward_rate", "Hacia abajo", "Hacia arriba")
  )

g3 <- g3_data %>%
  ggplot(aes(x = prop_lower_ses_cat, y = rate, fill = Direction)) +
  geom_col(position = "dodge", alpha = 0.8) +
  geom_errorbar(aes(ymin = rate - 1.96*se, ymax = rate + 1.96*se),
                position = position_dodge(width = 0.9), width = 0.25) +
  labs(title = "Imitaci√≥n hacia Abajo vs hacia Arriba (Q2-Q3)",
       subtitle = "Comparaci√≥n de patrones de imitaci√≥n por SES",
       x = "Proporci√≥n de vecinos de SES m√°s bajo",
       y = "Tasa de imitaci√≥n",
       fill = "Direcci√≥n",
       caption = "Solo quintiles Q2-Q3 | Barras de error: IC 95%") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(size = 14, face = "bold")) +
  scale_fill_manual(values = c("Hacia abajo" = "#2E8B57", "Hacia arriba" = "#4682B4"))

print(g3)

# GR√ÅFICO 4: Coeficientes de interacci√≥n por ciudad
print("Creando Gr√°fico 4: Heterogeneidad por ciudad")

if (length(resultados_ciudad) > 0) {
  g4_data <- map_dfr(resultados_ciudad, function(x) {
    data.frame(
      ciudad = x$ciudad,
      coef = x$coef_interaccion,
      se = x$se,
      n_obs = x$n_obs,
      p_value = x$p_value
    )
  }) %>%
    mutate(
      significativo = ifelse(p_value < 0.05, "Significativo", "No significativo"),
      ciudad = str_to_title(ciudad)
    )
  
  g4 <- g4_data %>%
    ggplot(aes(x = reorder(ciudad, coef), y = coef, fill = significativo)) +
    geom_col(alpha = 0.8) +
    geom_errorbar(aes(ymin = coef - 1.96*se, ymax = coef + 1.96*se), width = 0.25) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
    labs(title = "Efecto Wide Bridges por Ciudad",
         subtitle = "Coeficiente de interacci√≥n: same_ses √ó narrow_bridge",
         x = "Ciudad",
         y = "Coeficiente de interacci√≥n",
         fill = "Significancia",
         caption = "L√≠nea roja: efecto nulo | Barras de error: IC 95%") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          plot.title = element_text(size = 14, face = "bold")) +
    scale_fill_manual(values = c("Significativo" = "#E31A1C", "No significativo" = "#999999"))
  
  print(g4)
}

################################################################################
# PASO 7: IDEA 3 - AN√ÅLISIS CAUSAL CON IPTW (ACTUALIZADO)
################################################################################

print("\nPASO 7: IDEA 3 - AN√ÅLISIS CAUSAL CON IPTW (ACTUALIZADO)")
print("=======================================================")

# Funci√≥n corregida para crear datos a nivel ego
create_ego_level_data <- function(data, global_ses_median) {
  
  print("Creando datos a nivel de ego con mediana global...")
  
  # Verificar que la mediana global es un √∫nico valor num√©rico
  if (length(global_ses_median) != 1 || !is.numeric(global_ses_median)) {
    stop("'global_ses_median' debe ser un √∫nico valor num√©rico.")
  }
  
  ego_data <- data %>%
    filter(!is.na(ego_id) & !is.na(ses_ego)) %>%
    group_by(ego_id) %>%
    summarise(
      # Variable dependiente: proporci√≥n de vecinos con la misma escuela
      prop_same_school = mean(mismo_post_post, na.rm = TRUE),
      
      # Variable de tratamiento: ¬øtiene el ego al menos 1 vecino en la misma escuela?
      has_same_school_neighbor = as.numeric(any(mismo_post_post == 1, na.rm = TRUE)),
      
      # Variable de resultado: ¬øtiene ego SES alto? (usando mediana global)
      ego_high_ses = as.numeric(ses_ego[1] > global_ses_median),
      
      # Variables para matching
      ses_ego = ses_ego[1],
      sexo_ego = sexo_ego[1],
      edad_ego = edad_ego[1],
      ses_mean = ses_mean[1],
      ses_sd = ses_sd[1],
      shannon_index = shannon_index[1],
      network_size = network_size[1],
      city = city[1],
      reference_year = reference_year[1],
      
      n_alters = n(),
      
      .groups = "drop"
    ) %>%
    filter(
      !is.na(ses_ego), !is.na(sexo_ego), !is.na(edad_ego),
      !is.na(city), !is.na(reference_year),
      n_alters >= 2
    )
  
  return(ego_data)
}

# Verificar que los datos existen
if (!exists("d_post") || !exists("d_matric")) {
  print("ERROR: Los datos d_post y d_matric no existen")
} else {
  
  tryCatch({
    # Calcular las medianas globales PRIMERO
    print("Calculando medianas globales de SES...")
    global_median_post <- median(d_post$ses_ego, na.rm = TRUE)
    global_median_matric <- median(d_matric$ses_ego, na.rm = TRUE)
    
    print(paste("Mediana global POST:", round(global_median_post, 4)))
    print(paste("Mediana global MATRIC:", round(global_median_matric, 4)))
    
    # Crear datos a nivel ego
    ego_data_post <- create_ego_level_data(d_post, global_median_post)
    ego_data_matric <- create_ego_level_data(d_matric, global_median_matric)
    
    print("Datos a nivel de ego creados exitosamente:")
    print(paste("POST-POST: N =", nrow(ego_data_post)))
    print(paste("POST-MATRIC: N =", nrow(ego_data_matric)))
    
    # Mostrar distribuciones
    print("Distribuci√≥n variable de tratamiento (has_same_school_neighbor) en POST-POST:")
    print(table(ego_data_post$has_same_school_neighbor))
    
    print("Distribuci√≥n variable dependiente (ego_high_ses) en POST-POST:")
    print(table(ego_data_post$ego_high_ses))
    
  }, error = function(e) {
    print("ERROR durante la creaci√≥n de datos a nivel de ego:")
    print(e$message)
  })
}

# An√°lisis causal con IPTW
if (exists("ego_data_post") && nrow(ego_data_post) > 0) {
  
  print("\n=== AN√ÅLISIS CAUSAL CON IPTW ===")
  
  # Limpiar datos para matching
  vars_for_matching <- c("has_same_school_neighbor", "ego_high_ses", "ses_ego", "sexo_ego", "edad_ego", "city", "reference_year")
  ego_data_post_clean <- ego_data_post[complete.cases(ego_data_post[, vars_for_matching]), ]
  
  print(paste("Observaciones despu√©s de limpiar NAs:", nrow(ego_data_post_clean)))
  
  # Verificar variaci√≥n en tratamiento y resultado
  print("Tabla de tratamiento:")
  print(table(ego_data_post_clean$has_same_school_neighbor))
  print("Tabla de resultado:")
  print(table(ego_data_post_clean$ego_high_ses))
  
  # Aplicar IPTW
  tryCatch({
    print("Iniciando Ponderaci√≥n por Inverso de la Probabilidad de Tratamiento (IPTW)...")
    
    # F√≥rmula robusta para propensity score
    formula_ps <- has_same_school_neighbor ~ ses_ego + I(ses_ego^2) + 
      edad_ego + I(edad_ego^2) + 
      sexo_ego + 
      factor(city) + 
      ses_ego:factor(city) + 
      reference_year
    
    # Calcular los pesos usando WeightIt
    W.out <- weightit(
      formula_ps,
      data = ego_data_post_clean,
      method = "ps",
      estimand = "ATT"
    )
    
    print("Ponderaci√≥n completada. Resumen de los pesos:")
    print(summary(W.out))
    
    # Diagnosticar el balance
    print("Evaluando el balance de las covariables:")
    bal_table <- bal.tab(
      W.out, 
      un = TRUE,
      thresholds = c(m = 0.1)
    )
    print(bal_table)
    
    # A√±adir los pesos al dataframe
    ego_data_post_clean$iptw_weights <- W.out$weights
    
    # Modelo final ponderado
    print("Ajustando modelo GLM ponderado...")
    weighted_model <- glm(
      ego_high_ses ~ has_same_school_neighbor,
      data = ego_data_post_clean,
      family = "binomial",
      weights = iptw_weights
    )
    
    print("RESULTADOS DEL MODELO PONDERADO (IPTW):")
    print(summary(weighted_model))
    
    # Errores est√°ndar robustos
    if(require(lmtest, quietly = TRUE) && require(sandwich, quietly = TRUE)){
      print("Calculando errores est√°ndar robustos:")
      robust_results <- coeftest(weighted_model, vcov. = vcovHC)
      print(robust_results)
      
      coef_weighted <- robust_results[2, "Estimate"]
      p_weighted <- robust_results[2, "Pr(>|z|)"]
    } else {
      print("Usando errores est√°ndar est√°ndar...")
      coef_weighted <- coef(weighted_model)["has_same_school_neighbor"]
      p_weighted <- summary(weighted_model)$coefficients["has_same_school_neighbor", "Pr(>|z|)"]
    }
    
    print(paste("Coeficiente del tratamiento (ATT):", round(coef_weighted, 4)))
    print(paste("P-valor:", round(p_weighted, 4)))
    
    # Interpretaci√≥n
    print("\nINTERPRETACI√ìN:")
    print("Revisa la tabla de balance arriba. Solo conf√≠a en el resultado si las variables est√°n balanceadas.")
    
    if (p_weighted < 0.05) {
      print("‚Üí HAY EVIDENCIA DE INFLUENCIA SOCIAL (si balance es bueno)")
      print("‚Üí Tener vecinos con misma escuela AFECTA el SES del ego")
    } else {
      print("‚Üí NO hay evidencia de influencia social")
      print("‚Üí Los patrones son principalmente homofilia")
    }
    
    # An√°lisis de sensibilidad
    print("\n=== AN√ÅLISIS DE SENSIBILIDAD ===")
    print("Probando diferentes umbrales para SES alto...")
    
    sensitivity_results <- data.frame(
      percentile = numeric(),
      coefficient = numeric(),
      p_value = numeric(),
      stringsAsFactors = FALSE
    )
    
    for(percentile in c(0.4, 0.5, 0.6, 0.7)) {
      
      ego_data_post_clean$ego_high_ses_alt <- as.numeric(
        ego_data_post_clean$ses_ego > quantile(ego_data_post_clean$ses_ego, percentile, na.rm = TRUE)
      )
      
      tryCatch({
        model_alt <- glm(ego_high_ses_alt ~ has_same_school_neighbor,
                         data = ego_data_post_clean,
                         family = "binomial", 
                         weights = iptw_weights)
        
        coef_alt <- coef(model_alt)["has_same_school_neighbor"]
        p_alt <- summary(model_alt)$coefficients["has_same_school_neighbor", "Pr(>|z|)"]
        
        sensitivity_results <- rbind(sensitivity_results, data.frame(
          percentile = percentile,
          coefficient = coef_alt,
          p_value = p_alt
        ))
        
        print(paste("Percentil", percentile, "- Coef:", round(coef_alt, 4), "- P:", round(p_alt, 4)))
        
      }, error = function(e) {
        print(paste("Error en percentil", percentile, ":", e$message))
      })
    }
    
    print("\nRESUMEN DE SENSIBILIDAD:")
    if (nrow(sensitivity_results) > 0) {
      significant_results <- sum(sensitivity_results$p_value < 0.05, na.rm = TRUE)
      print(paste("Resultados significativos:", significant_results, "de", nrow(sensitivity_results)))
      
      if (significant_results >= 3) {
        print("‚Üí RESULTADO ROBUSTO: Efecto consistente across diferentes umbrales")
      } else {
        print("‚Üí RESULTADO SENSIBLE: Efecto depende del umbral de SES alto")
      }
    }
    
  }, error = function(e) {
    print("Error durante el an√°lisis IPTW:")
    print(e$message)
  })
  
} else {
  print("No se ejecut√≥ el an√°lisis causal porque ego_data_post no existe o est√° vac√≠o.")
}

################################################################################
# PASO 8: S√çNTESIS Y CONCLUSIONES
################################################################################

print("\nPASO 8: S√çNTESIS Y CONCLUSIONES")
print("==============================")

# Funci√≥n para crear tabla resumen
create_results_summary <- function() {
  
  results_data <- data.frame(
    Hip√≥tesis = character(0),
    Coeficiente = numeric(0),
    Significancia = character(0),
    Interpretaci√≥n = character(0),
    stringsAsFactors = FALSE
  )
  
  # Agregar resultados de Ideas 1 y 2
  if (exists("m_idea1_basic") && !is.null(m_idea1_basic)) {
    results_data <- rbind(results_data, data.frame(
      Hip√≥tesis = "H1: Homofilia √ó Composici√≥n Egohood",
      Coeficiente = round(coef(m_idea1_basic)["interaction_homofilia"], 4),
      Significancia = ifelse(summary(m_idea1_basic)$coefficients["interaction_homofilia", "Pr(>|z|)"] < 0.05, "S√≠", "No"),
      Interpretaci√≥n = ifelse(coef(m_idea1_basic)["interaction_homofilia"] < 0, "Efecto salience", "Efecto conformidad"),
      stringsAsFactors = FALSE
    ))
  }
  
  if (exists("m_wide_bridges") && !is.null(m_wide_bridges)) {
    results_data <- rbind(results_data, data.frame(
      Hip√≥tesis = "H1 (Wide Bridges): same_ses √ó narrow_bridge",
      Coeficiente = round(coef(m_wide_bridges)["same_ses_quintil:narrow_bridge_same_ses"], 4),
      Significancia = ifelse(summary(m_wide_bridges)$coefficients["same_ses_quintil:narrow_bridge_same_ses", "Pr(>|z|)"] < 0.1, "S√≠", "No"),
      Interpretaci√≥n = ifelse(coef(m_wide_bridges)["same_ses_quintil:narrow_bridge_same_ses"] < 0, "Valida Centola", "No valida Centola"),
      stringsAsFactors = FALSE
    ))
    
    results_data <- rbind(results_data, data.frame(
      Hip√≥tesis = "H1 (Diversidad): ses_diversity_index",
      Coeficiente = round(coef(m_wide_bridges)["ses_diversity_index"], 4),
      Significancia = ifelse(summary(m_wide_bridges)$coefficients["ses_diversity_index", "Pr(>|z|)"] < 0.05, "S√≠", "No"),
      Interpretaci√≥n = ifelse(coef(m_wide_bridges)["ses_diversity_index"] < 0, "Complex contagion", "Simple contagion"),
      stringsAsFactors = FALSE
    ))
  }
  
  if (exists("m_idea2_q2q3") && !is.null(m_idea2_q2q3)) {
    results_data <- rbind(results_data, data.frame(
      Hip√≥tesis = "H2: Imitaci√≥n hacia abajo √ó Composici√≥n",
      Coeficiente = round(coef(m_idea2_q2q3)["interaction_downward"], 4),
      Significancia = ifelse(summary(m_idea2_q2q3)$coefficients["interaction_downward", "Pr(>|z|)"] < 0.05, "S√≠", "No"),
      Interpretaci√≥n = ifelse(coef(m_idea2_q2q3)["interaction_downward"] < 0, "Info valiosa escasa", "No efecto"),
      stringsAsFactors = FALSE
    ))
    
    results_data <- rbind(results_data, data.frame(
      Hip√≥tesis = "H2 (Barrios aventajados): downward √ó advantaged",
      Coeficiente = round(coef(m_idea2_q2q3)["interaction_downward_advantaged"], 4),
      Significancia = ifelse(summary(m_idea2_q2q3)$coefficients["interaction_downward_advantaged", "Pr(>|z|)"] < 0.05, "S√≠", "No"),
      Interpretaci√≥n = ifelse(coef(m_idea2_q2q3)["interaction_downward_advantaged"] > 0, "Efecto barrios aventajados", "No efecto"),
      stringsAsFactors = FALSE
    ))
  }
  
  # Agregar resultado causal si existe
  if (exists("coef_weighted") && exists("p_weighted")) {
    results_data <- rbind(results_data, data.frame(
      Hip√≥tesis = "H3: Efecto causal (IPTW)",
      Coeficiente = round(coef_weighted, 4),
      Significancia = ifelse(p_weighted < 0.05, "S√≠", "No"),
      Interpretaci√≥n = ifelse(p_weighted < 0.05, "Influencia social", "Solo homofilia"),
      stringsAsFactors = FALSE
    ))
  }
  
  return(results_data)
}

# Crear y mostrar tabla resumen
results_summary <- create_results_summary()
print("TABLA RESUMEN DE RESULTADOS:")
print(results_summary)

# CONCLUSIONES PRINCIPALES
print("\nCONCLUSIONES PRINCIPALES:")
print("=========================")

print("1. VALIDACI√ìN DE LA TEOR√çA DE COMPLEX CONTAGIONS:")
if (exists("m_wide_bridges") && !is.null(m_wide_bridges)) {
  interaction_coef <- coef(m_wide_bridges)["same_ses_quintil:narrow_bridge_same_ses"]
  diversity_coef <- coef(m_wide_bridges)["ses_diversity_index"]
  
  if (interaction_coef < 0) {
    print("   ‚úì CONFIRMADA: Homofilia m√°s fuerte con narrow bridges")
    print("   ‚úì La elecci√≥n escolar funciona como COMPLEX CONTAGION")
    print("   ‚úì Validaci√≥n emp√≠rica de Centola & Macy (2007)")
  } else {
    print("   ‚úó NO confirmada: Narrow bridges no amplifican homofilia")
  }
  
  if (diversity_coef < 0) {
    print("   ‚úì CONFIRMADA: Diversidad SES reduce homofilia")
    print("   ‚úì Evidencia de 'complex contagion environment'")
  }
}

print("\n2. HIP√ìTESIS DE IMITACI√ìN HACIA ABAJO:")
if (exists("m_idea2_q2q3") && !is.null(m_idea2_q2q3)) {
  downward_coef <- coef(m_idea2_q2q3)["interaction_downward"]
  
  if (abs(downward_coef) < 0.01) {
    print("   ‚úó NO confirmada: Sin efecto de informaci√≥n valiosa de fuentes escasas")
  } else {
    print("   ‚úì CONFIRMADA: Efecto de informaci√≥n valiosa de fuentes escasas")
  }
}

print("\n3. EFECTOS CAUSALES VS HOMOFILIA:")
if (exists("coef_weighted") && exists("p_weighted")) {
  if (p_weighted < 0.05) {
    print("   ‚úì CONFIRMADA: Hay influencia social genuina")
    print("   ‚úì Los patrones no son solo homofilia")
  } else {
    print("   ‚úó NO confirmada: Efectos son principalmente homofilia")
  }
}

print("\n4. IMPLICACIONES PARA POL√çTICA EDUCATIVA:")
print("   - Las redes sociales importan para la elecci√≥n escolar")
print("   - Los efectos var√≠an seg√∫n la composici√≥n del egohood")
print("   - Las estrategias de difusi√≥n deben considerar wide bridges")
print("   - La diversidad SES afecta la propagaci√≥n de innovaciones")

print("\n5. CONTRIBUCIONES TE√ìRICAS:")
print("   - Primera validaci√≥n emp√≠rica de complex contagions en educaci√≥n")
print("   - Evidencia de que la topology de redes sociales importa")
print("   - Conexi√≥n entre sociolog√≠a anal√≠tica y pol√≠tica educativa")
print("   - Metodolog√≠a IPTW robusta para an√°lisis causal")

################################################################################
# GUARDAR RESULTADOS
################################################################################

print("\nGUARDANDO RESULTADOS:")
print("====================")

# Guardar modelos principales
models_to_save <- list()
if (exists("m_idea1_basic")) models_to_save$m_idea1_basic <- m_idea1_basic
if (exists("m_wide_bridges")) models_to_save$m_wide_bridges <- m_wide_bridges
if (exists("m_idea2_q2q3")) models_to_save$m_idea2_q2q3 <- m_idea2_q2q3
if (exists("m_idea2_upward")) models_to_save$m_idea2_upward <- m_idea2_upward

if (length(models_to_save) > 0) {
  save(list = names(models_to_save), file = "modelos_complex_contagions_completo.RData")
  print("- Modelos principales guardados")
}

# Guardar resultados causales
if (exists("ego_data_post_clean") && exists("weighted_model")) {
  save(ego_data_post_clean, weighted_model, W.out, file = "resultados_causales_iptw.RData")
  print("- Resultados causales IPTW guardados")
}

# Guardar tabla resumen
write.csv(results_summary, "tabla_resumen_resultados_final.csv", row.names = FALSE)
print("- Tabla resumen exportada")

# Guardar gr√°ficos
if (exists("g1")) {
  if (!dir.exists("graficos")) dir.create("graficos")
  
  ggsave("graficos/wide_bridges_validation.png", g1, width = 12, height = 8, dpi = 300)
  print("- Gr√°fico wide bridges guardado")
}

if (exists("g2")) {
  ggsave("graficos/diversity_effects.png", g2, width = 12, height = 8, dpi = 300)
  print("- Gr√°fico diversidad guardado")
}

if (exists("g3")) {
  ggsave("graficos/upward_vs_downward.png", g3, width = 12, height = 8, dpi = 300)
  print("- Gr√°fico imitaci√≥n guardado")
}

if (exists("g4")) {
  ggsave("graficos/heterogeneity_by_city.png", g4, width = 12, height = 8, dpi = 300)
  print("- Gr√°fico heterogeneidad guardado")
}

print("\nArchivos guardados:")
print("- modelos_complex_contagions_completo.RData")
print("- resultados_causales_iptw.RData")
print("- tabla_resumen_resultados_final.csv")
print("- graficos/wide_bridges_validation.png")
print("- graficos/diversity_effects.png")
print("- graficos/upward_vs_downward.png")
print("- graficos/heterogeneity_by_city.png")

print("\n" + paste(rep("=", 80), collapse = ""))
print("AN√ÅLISIS COMPLETO DE COMPLEX CONTAGIONS EN ELECCI√ìN ESCOLAR")
print("Ideas 1, 2 y 3 implementadas con metodolog√≠a IPTW robusta")
print("Validaci√≥n emp√≠rica de Centola & Macy (2007) completada")
print(paste(rep("=", 80), collapse = ""))

################################################################################
# FIN DEL AN√ÅLISIS
################################################################################